---
title: "HW 3"
author: Sarath Haridas, Abhinav Kumar, Tarun Newton, Pranav Pura Lingaraju, Kavya Puthuvaya, Pranav Srinivas Nagavelli
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: yes
urlcolor: blue
---


# Business Problem

Michael Warnken (Senior Director of e-Commerce & General Manager) and Roselie Vaughn (Director of Customer Digital Experience) of Sun Country Airlines plan to develop more robust, data-driven customer insights in order to better structure their marketing efforts for the customers. To do so, they would like to better understand their customers. 


## Why Clustering?

We believe that by identifying segments of customers, the marketing efforts can be targeted to customer segments instead of tailoring the effort to individual customers. The segements could be identified in a way that ensures that the customers within the segment exhibit similar behaviour and customers across segments behave differently. 

Given that there are no existing segments available, we use an unsupervised clustering alogrithm to identify the segments. 


# Data Cleaning

We import the data and explore the same to understand its structure, and clean the data to make it ready for analyses. 


## Data Import 

Loading libraries

```{r global_options, warning = FALSE, message = FALSE}
library(data.table)
library(lubridate)
library(dplyr)
library(stringr)
library(cluster)
library(ggplot2)
library(ggfortify)
library(stats)
library(factoextra)
library(naniar)
```

Importing data and removing rows that are completely duplicated. 

_Assumption_

Since the data is at a trip-passenger level, duplicate records are data entry issues

```{r eval = TRUE, echo = TRUE}
# Data import
sun <- fread('SunCountry.csv', header = TRUE, stringsAsFactors = FALSE)

str(sun)

# Removing row level duplicates
sun_undup <- sun[!duplicated(sun), ]
```

Analyzing the structure of the data

```{r}
summary(sun_undup)
dim(sun_undup)
```

We have data for 3.3 million observations and 26 columns that captures travel information for each passenger for each trip. There are some irregularities that are clearly identifiable from certain columns

_Findings_

* Age and birthdate: We observe that the age has values that are missing. And, there are observations with negative age, which might have arised from a data entry issue at the birthdate column. We also have observations for age that are over 100 years old. 
* GenderCode: We observe null values, and there are records where the gender is marked as U (Unknown)
* UflyRewardsNumber and CardHolder: We observe that there are multiple NAs for these two. This could either be because the customer did not have an SC credit card or that he didn't use this SC Credit card for the specific transaction. For the purpose of this analysis, we assume that the customer did not have an SC credit card. 
* MarketingAirlineCode: There are airline codes capturing for other airlines as well. SY refers to Sun Country, and our analysis is going to focus only for Sun Country airlines
* BaseFareAmount and TotalDocAmount: We observe that there are records where the BaseFareAmount and the TotalFare amount are over USD 10,000. The maximum [ticket price](https://www.suncountry.com/booking/search.html) that Sun Country flies for Domestic in 2018 is USD 800. Therefore, we have to treat these extreme values before using them in any analysis 

We subset the data for only Sun Country flights by filtering for records having the MarketingAirlineCode as SY. Because we are primarily interested in identifying customer patterns for Sun Country airlines only. 

```{r}
sun_undup <- sun_undup %>% filter(MarketingAirlineCode == 'SY')
```


## Missing Value Treatment

Firstly, we try to identify the sparsity of the missing values for the columns. 

```{r}
gg_miss_upset(sun_undup, nsets = 4)
```

_Description_

The graph above captures the number of missing records in the data, and also captures the interaction between the missing values. The linkages at the bottom capture which variables are missing in common for the corresponding bar. For example, the first bar has around 258K missing values for both the UFlyRewardsNumber and the CardHolder variables. 

_Interpretation_

We observe that the missing values are present for 4 columns only. Out of which, the missing values for UflyRewardsNumber and CardHolder are present across the same set of records, and the missing values for Age and BirthDateId are present across the same set of records. 


### Treating for UFlyRewardsNumber and CardHolder

The missing values for the UFlyRewardsNumber and CardHolder are present across the same set of records. Hence, the missing values can be replaced with dummy values for both the columns. 

For the UFlyRewardsNumber, we can replace the missing values with 0 and CardHolder with "NA". 

```{r}
sun_undup$UFlyRewardsNumber <- ifelse(is.na(sun_undup$UFlyRewardsNumber), 0, 
                                      sun_undup$UFlyRewardsNumber)
sun_undup$CardHolder <- ifelse(is.na(sun_undup$CardHolder), "NA", 
                               sun_undup$CardHolder)
```


### Treating for Age and BirthDate

Before treating for the missing value of age, we must analyze whether the variables are missing at random or if there exists a pattern that would help us impute the missing values. 

```{r}
# Comparing missingness of age with other variables
# Service Start Date
table("Age Missing" = is.na(sun_undup$Age), 
      "Service Start Month" = lubridate::month(sun_undup$ServiceStartDate))

# PNR Create Date
table("Age Missing" = is.na(sun_undup$Age), 
      "PNR Create Month" = lubridate::month(sun_undup$PNRCreateDate))

# Gender Code
table("Age Missing" = is.na(sun_undup$Age), 
      "Gender Code" = sun_undup$GenderCode)

# Booking Channel
table("Age Missing" = is.na(sun_undup$Age), 
      "Booking Channel" = sun_undup$BookingChannel)
```

_Interpretation_

Similarly, comparing across all the other variables we are unable to observe any pattern with respect to any variable. Therefore, we infer that the values for age are missing at random, and can be removed. Also, there are only 38 records for which the Gender is Unknown ("U"). Therefore, these records with unknown Gender code can also be removed.

```{r}
sun_undup_age_rm <- sun_undup %>% filter(!is.na(Age) & GenderCode != 'U')
```

_Conclusion_

* The missing values for the UFlyRewardsNumber and CardHolder have been replaced with dummy values. 
* The missing values for the Age column were removed as they were identified to be missing at random


## Outlier Treatment

We had identified outliers in the Age, TotalDocAmount and the BaseFareAmount. 


### Treating for Age

For age, there are only 4 records where the age is negative. Also, there are only a few records where age is > 100. There cannot be any person who has a negative age. Also, there are hardly people over the age of 100 who travel. Hence, these have to be data entry errors. These also have to be treated akin to the missing age approach. Also, since there are only a few records, we remove these records where age is beyond these limits instead of replacing them with median. 
_Assumption_

The data for which ages are out of the threshold are also missing at random, and therefore, removing them would not impact any inferences drawn from the data. 

```{r}
sun_undup_age_rm <- sun_undup %>% 
  filter(Age > 0 & Age <= 100) 
```

_Description_ 


### Treating for amounts

We identify the univariates for the variables TotalDocAmount and the BaseFareAmount. As mentioned above, the current highest ticket price in the Sun Country website is around USD 800. Therefore, we can cap the prices at the 99.9^th^ percentile for those records with higher prices. 

The 99.9^th^ percentiles for these 2 variables are USD 1,364 and USD 1,258. 

```{r}
quantile(sun_undup_age_rm$BaseFareAmt, probs = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 
                                        0.9, 0.95, 0.99, 0.995, 0.999, 1))
# 1258
quantile(sun_undup_age_rm$TotalDocAmt, probs = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 
                                        0.9, 0.95, 0.99, 0.995, 0.999, 1))
# 1364

sun_fin <- sun_undup_age_rm %>% 
  mutate(BaseFareAmt = ifelse(BaseFareAmt > 1258, 1258, BaseFareAmt), 
         TotalDocAmt = ifelse(TotalDocAmt > 1364, 1364, TotalDocAmt))
```

_Description_

We also want to make sure we capture those customers who have had their first leg of journey from Sun Country. 

_Assumption_

It is possible that certain customers flew another airline and used Sun Country as a connector. We next filter our data to include only those PNR IDs for which Sun Country was the first flight. We identify the leg of journey using the Coupon Sequence Number and filter out PNR IDs which don't start with coupon sequence number of 1. 

```{r}
pnr <- sun_fin %>% 
  group_by(PNRLocatorID) %>% 
  summarise(min_cpn = min(CouponSeqNbr)) %>% 
  filter(min_cpn == 1)
```

_Conclusion_

* The values for BaseFareAmount and TotalDocAmount were capped at their respective 99.9^th^ percentile values
* Records where Age was negative or over 100 were assumed as data entry errors and treated as missing at Random and removed as per the Missing Value Treatment approach


# Identifying Unique Customer

To identify customers, we need a unique key to identify customers. We could use the UFlyRewardsNumber to identify unique members. But we have no unique identifier for a non-member. Therefore, we have to identify a unique member identifier across all customers. 

While there was no single column or combination of columns that was able to identify a unique member for the whole dataset, we observed that the combination of EncryptedName, BirthDateId and GenderCode was mostly unique. Hence, we assume that the unique identifier for a member is this combination. 

We create a new column pkey which is a concatenation of the values in these columns. We use the pkey for the analysis going forward. 

```{r}
sun_fin_out <- sun_fin %>% 
  filter(PNRLocatorID %in% pnr$PNRLocatorID) %>% 
  mutate(pkey = paste(EncryptedName, birthdateid, GenderCode, sep = "-"))
```


# Data Exploration

We explore the data to understand common patterns or trends that would help us in identifying customer groups. 


## Age distribution

```{r}
ggplot(data = sun_fin_out) +
  aes(x = Age) +
  geom_histogram(bins = 19, fill = "#2171b5") +
  labs(title = "Trip Distribution according to Age",
       x = "Age",
       y = "No of Trips") +
  theme_minimal()
```


## Trip count for top 10 destinations

```{r}
sun_end_city <- sun_fin_out %>% 
  group_by(ServiceEndCity) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% head(11)

sub <- sun_end_city$ServiceEndCity[c(2:11)]

sun_end_city_top <- sun_fin_out  %>% 
  filter(ServiceEndCity %in% sub) 

ggplot(data = sun_end_city_top) +
  aes(x = ServiceEndCity) +
  geom_bar(fill = "#08519c") +
  labs(title = "Top 10 Destinations According to No of Trips",
       x = "Destination City",
       y = "No of Trips") +
  theme_minimal()
```


## Trip distribution according to month


```{r}
sun_fin_out_qtr <-sun_fin_out
sun_fin_out_qtr$qtr <- lubridate::quarter(sun_fin_out_qtr$ServiceStartDate)
sun_fin_out_qtr <- sun_fin_out_qtr %>% 
  filter(ServiceStartCity %in% "MSP")

date_dataset <- sun_fin_out_qtr %>% 
  group_by(month=lubridate::month(ServiceStartDate)) %>% 
  summarise(count = n())
date_dataset$month_abb <- month.abb[date_dataset$month]
date_dataset$month_abb_fac = factor(date_dataset$month_abb, levels = month.abb)

ggplot(data = date_dataset) +
  aes(x = month_abb_fac, weight = `count`) +
  geom_bar(fill = "#6a51a3") +
  labs(title = "Trip Distribution According to Month",
       x = "Month",
       y = "No of Trip") +
  theme_minimal()
```


# Data Transformation

To identify groups of similar customers, we need to create customer level clusters. To do so, we identify attributes that pertain to every individual customer. We aggregate from the trip level data to a customer level to arrive at our final dataset. 


## Analytical Dataset Structure

The table below details the list of columns that are captured in the analtyical dataset. 

Column Name           | Description                             | Comments
--------------------- | -------------------------------------   | ---------------------
pkey                  | Primary Key for the customer            |
GenderCode            | Gender                                  | 1 for Male
age                   | Age                                     |
amount                | TotalDocAmount Paid by the customer     |
bkng_chnl_out         | Booking Channel - Outside Booking       |
bkng_chnl_reserv      | Booking Channel - Reservation Booking   |
bkng_chnl_syvac       | Booking Channel - SY Vacation Booking   |
bkng_chnl_tour        | Booking Channel - Tour Operator         |
bkng_chnl_web         | Booking Channel - SY Website Booking    |
booked_coach_travel   | Booked Class - Coach                    |
booked_fc_travel      | Booked Class - First Class              |
card_holder           | SY Card Holder                          |
city_per_trip         | Number of cities visited                |
no_booking            | Number of tickets booked                | Distinct PNR Count
travel_coach_travel   | Travelled Class - Coach                 |
travel_fc_travel      | Travelled Class - First Class           |
avg_min_dbd           | Min Days Before Departure Tickets Booked|
avg_max_dbd           | Max Days Before Departure Tickets Booked|
avg_len_stay          | Avg Length of Stay if Round Trip        |
min_len_stay          | Min Length of Stay if Round Trip        |
max_len_stay          | Max Length of Stay if Round Trip        |



## Creating the Analytical Dataset 

_Description_

Since the amount of data involved is huge, instead of aggregating the data at once, we aggregate individual columns and later merge them together. In this process, we remove the redundant data frames as well to conserve space. At the end of this approach, all the intermediate tables are removed, and only the required tables remain, which include the raw data and the customer level analytical dataset. 

Post the join, we convert the GenderCode column to a numeric by replacing the Male with 1 and Female with 0. This is to ensure that we are able to run our algorithms like k-Means or Hierarchical clustering. 

_Assumption_

* The customer behaviour is the same across the 2 years under consideration
* The customer behaviour will remain stable for the foreseeable future and the results of the analysis can be generalized for the current calendar year as well

```{r}
rm(sun)
rm(sun_undup)
rm(sun_undup_age_rm)
rm(pnr)
rm(sun_fin)
rm(date_dataset)
rm(sun_end_city)
rm(sun_end_city_top)
rm(sun_fin_out_qtr)
rm(sub)
```

```{r eval = FALSE}
# Getting Age
cust_raw_age <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(age = max(Age))

# Getting Number of Bookings
cust_raw_no_booking <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(no_booking = n_distinct(PNRLocatorID))

# Getting Number of Bookings with Coach
cust_raw_booking_coach_travel <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(booked_coach_travel = sum(BkdClassOfService == 'Coach'))

# Getting Number of Bookings with First Class
cust_raw_booking_fc_travel <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(booked_fc_travel = sum(BkdClassOfService == 'First Class'))

# Getting Number of Travels in Coach
cust_raw_travel_coach_travel <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(travel_coach_travel = sum(BkdClassOfService == 'Coach'))

# Getting Number of Travels in First Class
cust_raw_travel_fc_travel <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(travel_fc_travel = sum(BkdClassOfService == 'First Class'))

# Getting TotalDocAmount Paid by the Traveller
cust_raw_amount <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(amount = sum(TotalDocAmt))

# Getting the Number of Cities the Visited
cust_raw_city <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(city_per_trip = n_distinct(ServiceEndCity))

# Getting the Number of Instances of Outside Booking
cust_raw_bkng_chnl_out <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(bkng_chnl_out = sum(BookingChannel == 'Outside Booking'))

# Getting the Number of Instances of Reservations Booking
cust_raw_bkng_chnl_reserv <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(bkng_chnl_reserv = sum(BookingChannel == 'Reservations Booking'))

# Getting the Number of Instances of Tour Operator Booking
cust_raw_bkng_chnl_tour <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(bkng_chnl_tour = sum(BookingChannel == 'Tour Operator Portal'))

# Getting the Number of Instances of SY Vacation Booking
cust_raw_bkng_chnl_syvac <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(bkng_chnl_syvac = sum(BookingChannel == 'SY Vacation'))

# Getting the Number of Instances of SCA Website Booking
cust_raw_bkng_chnl_web <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(bkng_chnl_web = sum(BookingChannel == 'SCA Website Booking'))

# Identifying whether the Customer is a Card Holder
cust_raw_card_holder <- sun_fin_out %>% group_by(pkey, GenderCode) %>%
  summarise(card_holder = sum(!is.na(CardHolder)))

# Identifying the Days Before Departure and Length of Stay Metrics
cust_raw_datediff <- sun_fin_out %>% group_by(pkey, GenderCode, PNRLocatorID) %>%
  summarise(min_dbd = min(date_diff), max_dbd = max(date_diff)) %>% 
  mutate(len_stay = max_dbd - min_dbd) %>% 
  group_by(pkey, GenderCode) %>% 
  summarize(avg_min_dbd = mean(min_dbd), avg_max_dbd = mean(max_dbd), 
            avg_len_stay = mean(len_stay), min_len_stay = min(len_stay), 
            max_len_stay = max(len_stay))

# Merging Individual Datasets and Dropping Redundnat datasets
cust_raw_1 <- merge(cust_raw_age, cust_raw_amount)

rm(cust_raw_age)
rm(cust_raw_amount)

cust_raw_2 <- merge(cust_raw_1, cust_raw_bkng_chnl_out)

rm(cust_raw_1)
rm(cust_raw_bkng_chnl_out)

cust_raw_3 <- merge(cust_raw_2, cust_raw_bkng_chnl_reserv)

rm(cust_raw_2)
rm(cust_raw_bkng_chnl_reserv)

cust_raw_4 <- merge(cust_raw_3, cust_raw_bkng_chnl_syvac)

rm(cust_raw_3)
rm(cust_raw_bkng_chnl_syvac)

cust_raw_5 <- merge(cust_raw_4, cust_raw_bkng_chnl_tour)

rm(cust_raw_4)
rm(cust_raw_bkng_chnl_tour)

cust_raw_6 <- merge(cust_raw_5, cust_raw_bkng_chnl_web)

rm(cust_raw_5)
rm(cust_raw_bkng_chnl_web)

cust_raw_7 <- merge(cust_raw_6, cust_raw_booking_coach_travel)

rm(cust_raw_6)
rm(cust_raw_booking_coach_travel)

cust_raw_8 <- merge(cust_raw_7, cust_raw_booking_fc_travel)

rm(cust_raw_7)
rm(cust_raw_booking_fc_travel)

cust_raw_9 <- merge(cust_raw_8, cust_raw_card_holder)

rm(cust_raw_8)
rm(cust_raw_card_holder)

cust_raw_10 <- merge(cust_raw_9, cust_raw_city)

rm(cust_raw_9)
rm(cust_raw_city)

cust_raw_11 <- merge(cust_raw_10, cust_raw_no_booking)

rm(cust_raw_10)
rm(cust_raw_no_booking)

cust_raw_12 <- merge(cust_raw_11, cust_raw_travel_coach_travel)

rm(cust_raw_11)
rm(cust_raw_travel_coach_travel)

cust_raw_13 <- merge(cust_raw_12, cust_raw_travel_fc_travel)

rm(cust_raw_12)
rm(cust_raw_travel_fc_travel)

cust_raw_fin_1 <- merge(cust_raw_13, cust_raw_datediff)

rm(cust_raw_13)
rm(cust_raw_datediff)

# One-Hot Encoding of GenderCode variable
cust_raw_fin_1$GenderCode <- ifelse(cust_raw_fin_1$GenderCode == 'M', 1, 0)
```

```{r echo=FALSE}
cust_raw_fin_1 <- read.csv('cust_raw_fin_20181018.csv')
```

This is the anlaytical datataset that will be used for the clustering algorithm. The summary of it is below. 

```{r echo = TRUE}
# Top records for the Analytical Dataset
head(cust_raw_fin_1[, c(2:21)])
```


# Identifying Customer Segments

Post the creation of the analytical dataset, we have to segment the users into multiple groups. However, there are multiple approaches that can be leveraged to form clusters. There can be Hierarchical or Partitioning methods that can be used to create clusters. 

*Hierarchical methods*

Hierarchical clustering algorithms actually belong to 2 categories: 
* Bottom-up 
* Top-down 

Bottom-up algorithms treat each data point as a single cluster at the beginning and then successively merge pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called Agglomerative Clustering. This hierarchy is represented as a tree or dendrogram. 

Top-down algorithms flow in the opposite direction of the bottom-up algorithm. They start with all the data points and successively split the cluster into pairs until the end nodes are all the individual points. 

_conclusion_
Since each customer is unique and there exists no sub-segment level(s), we can assume that there exists no inherent hierarchical order in our data. Therefore, we can infer that the hierarchical methods are not suitable for cluster identification. We also substantiated this by performing hierarchical clustering and not observing satisfactory results. 

*Partitioning methods*
Partitioning clustering is used to classify observations into multiple groups based on their similarity. The paritioning algorithm works by iteratively re-allocating observations between clusters until a stable partition is reached. However, the number of clusters need to be specified by the user. Similarity is calculated based on distance calculations. 

We go ahead with the partitioning clustering method, with k-means as the first algorithm. 

## Rescaling data

_Description_

To apply distance based clustering, the first step is to rescale the numeric data columns ie., all numeric columns should have the same range of values. This process called normalization, will help us in handling columns that have varying scales. By normalizing, we rescale the data to a standardized scale, making the distance measures comparable. 

There can be two ways in which the data can be rescaled: 
* Min-Max Normalization -- The data is rescaled to a 0-1 scale
* Standardization -- The data is assumed to be normal and scaled to have a mean of 0 and a standard deviation of 1

The normalization method we chose is min-max normalization.


### Min_Max Normalization

In this normalization approach we bring all numeric columns to the range of 0 and 1 with 0 being the lowest value in the column and 1 being the highest value in the column. All other values are normalized based on the following formula:

        __ Yi = [Xi - min(X)]/[max(X) - min(X)] __

```{r}
normalize <- function(x){
  return ((x - min(x))/(max(x) - min(x)))}

idx <- sapply(cust_raw_fin_1, class) == "numeric" 

cust_raw_fin_1[,idx] <- sapply(cust_raw_fin_1[,idx], normalize)
```

Post normalization, we have to choose a clustering method and evaluate its performance. There are various partitioning clustering methods available, and we start with the k-means algorithm. 

**k-means Assumptions and Limitations**
* Can create clusters with a specific shape only -- Since we have no idea of how the actual clusters will look like, we can assume that the clusters we obtain out of the algorithm are spherical in shape as we use the Euclidian distance measure
* Can work with numerical data only -- Our dataset has only numerical clusters, and hence, there is no problem
* The number of clusters (k) needs to be specified before clustering -- We will evaluate the clustering performance and choose the clusters based on the results
* Highly sensitive to outliers -- Our data has been treated for outliers. Therefore, there would be no impact of outliers
* Cannot capture hierarchical structure -- Since, we have not observed any significant results out of hierarchical clustering, we can infer that there is no hierarchical structure
* Hard Clustering -- The customers are clustered into one group and one group only. It may be possible that a customer might belong to two different groups when his travel habits differ. But, given our original assumption that the behaviour is stable for the period under consideration, we can neglect this for the scope of this analysis. This assumption could be re-evaluated and restested in the next phase of the segmentation
* Convergence to local minima - k-means could converge to local minima instead of the global minima. The convergence should be evaluated by running multiple instances to identify whether similar results are being obtained across runs

The first step in the k-means algorithm is in choosing the value of k. To identify the value of k, we evaluate the clustering algorithm for different values of k and choose a k depending on the cluster performance. 


## Evaluating Clustering Performance

The clustering performance depends on the number of clusters we choose. The clusters formed should be such that there is high similarity within a cluster and low similarity between the clusters. 

We are looking at the two metrics to evaluate that the clustering performance:
* SSE (Sum of Squared Errors) -- SSE captures the sum of squared distance between each point and its centroid. Therefore, lower the SSE, higher the similarity between the point and its cluster 
* Silhoutte Coefficient -- SC is an alternative metric for cluster performance evaluation that is calculated based on the distance of a point to its cluster centroid and the nearest point outside of the cluster 

The performance we observed from the Silhouette coefficient was similar to the SSE. 


### Method 1: Elbow curve

```{r}
SSE_curve <- c()
for (n in 1:10) {
  k_cluster <- kmeans(cust_raw_fin_1[2:21], n)
  print(k_cluster$withinss)
  sse <- sum(k_cluster$withinss)
  SSE_curve[n] <- sse
}
plot(1:10, SSE_curve, type="l", xlab="Number of Clusters", ylab="SSE")
```

From the plot, we can see that the for K = 4 the SSE drop is steep and after K = 4 the SSE is almost constant.


### Method 2: Silhoutte Coefficient

For the calculation of the Silhouette Coefficient, we need to sample the dataset because computing the Silhoutte coefficient on the entire dataset is computationally tough. Therefore, we sampled 10000 records from the data, and based on the Silhoutte coefficient, generalized the K value for the entire dataset. 

We observed that the k value recommended by the Silhouette Coefficient is the same as SSE. 


## Creating Clusters

Applying the K-means algorithm on the transformed and normalized data with the number of clusters as 4. 

```{r}
set.seed(123)
k_cluster <- kmeans(cust_raw_fin_1[2:21], 4, nstart=25, iter.max=1000)
```


# Cluster Profiling

Customer segments provide clear information with respect to which customers fall under which segment. This understanding is crucial and will be leveraged for decision making. The output of a clustering algorithm doesn't explain what each cluster comprises of. If and only if the cluster composition is explained do the mathematically-derived clusters become business-consumable customer segments. 

After obtaining the clusters, it is imperative that we understand what observations fall under each cluster. This helps us in understanding the patterns that make up the cluster. Cluster profiling is the method by which we try to explain the similarity within clusters and identify patterns that make up the cluster. 

## Mapping clusters and raw data

The clusters identified are first mapped to the original dataset to identify what set of customers make up each cluster. 

```{r eval = FALSE}
cust_raw_fin_1$cluster_no <- k_cluster$cluster

# Merging the original dataset to get the cluster details
cluster_data <- merge(sun_fin_out, cust_raw_fin_1[,c("pkey","cluster_no")])
```

```{r echo = FALSE}
cluster_data <- fread('sun_w_clustersv3.0.csv', stringsAsFactors = FALSE)
```


# Conclusion
